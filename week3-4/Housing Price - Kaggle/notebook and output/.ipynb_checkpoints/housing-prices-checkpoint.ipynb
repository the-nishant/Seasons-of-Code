{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output(['ls', '../input/house-prices-advanced-regression-techniques']).decode('utf8')) #check the files available in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Now let's import and put the train and test datasets in  pandas dataframe\n",
    "\n",
    "train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the first five rows of the train dataset.\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the first five rows of the test dataset.\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the numbers of samples and features\n",
    "print(f'The train data size before dropping Id feature is : {train.shape}')\n",
    "print(f'The test data size before dropping Id feature is : {test.shape} ')\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop('Id', axis = 1, inplace = True)\n",
    "test.drop('Id', axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(f'\\nThe train data size after dropping Id feature is : {train.shape}')\n",
    "print(f'The test data size after dropping Id feature is : {test.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution  of SalePrice to ensure it is normal. If not, we would apply suitable transformation to make it normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "sns.distplot(train['SalePrice'] , fit=norm, ax=ax1);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "mu, sigma = norm.fit(train['SalePrice'])\n",
    "print(f'mu = {round(mu,2)} and sigma = {round(sigma,2)}')\n",
    "\n",
    "#Now plot the distribution\n",
    "ax1.legend([f'Normal dist. ($\\mu=$ {round(mu,2)} and $\\sigma=$ {round(sigma,2)} )'],\n",
    "            loc='best')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "res = stats.probplot(train['SalePrice'],plot=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed. A good choice would be the log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "\n",
    "#Check the new distribution\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "sns.distplot(train['SalePrice'] , fit=norm, ax=ax1);\n",
    "\n",
    "#Get the fitted parameters used by the function\n",
    "mu, sigma = norm.fit(train['SalePrice'])\n",
    "print(f'mu = {round(mu,2)} and sigma = {round(sigma,2)}')\n",
    "\n",
    "#Now plot the distribution\n",
    "ax1.legend([f'Normal dist. ($\\mu=$ {round(mu,2)} and $\\sigma=$ {round(sigma,2)} )'],\n",
    "            loc='best')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "res = stats.probplot(train['SalePrice'],plot=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew seems now corrected and the data appears more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the train and test data for handling missing values and other preprocessing. Also, we retain the number of train and test indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(f'all_data size is : {all_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find percentage of missing data\n",
    "missing_data = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "missing_data = missing_data[missing_data != 0].sort_values(ascending=False)\n",
    "\n",
    "#Plot the missing data\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(x=missing_data.index, y=missing_data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Features',fontsize=12)\n",
    "plt.ylabel('Percent of missing values', fontsize=12)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with missing values more than 20% missing values and other features related to them could be  dropped as the data available for them is too small to be used with confidence. However, as lasso regression will be used, they will automatically be discarded if they are not significant. Hence, all features are kept and imputed appropriately based on the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle missing values\n",
    "\n",
    "#Fill 'None' in categorical features to indicate absence of feature\n",
    "none_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType',\n",
    "                 'BsmtExposure', 'BsmtCond', 'BsmtQual','BsmtFinType2', 'BsmtFinType1', 'MasVnrType']\n",
    "all_data.fillna({col:'None' for col in none_features}, inplace =True)\n",
    "\n",
    "#Fill 0 in numerical features to indicate absence of feature\n",
    "zero_features = ['GarageYrBlt', 'MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "                 'BsmtUnfSF', 'GarageCars', 'GarageArea', 'TotalBsmtSF']\n",
    "all_data.fillna({col:0 for col in zero_features}, inplace =True)\n",
    "\n",
    "#Fill mode of some features(consider only the training set) where <1% data is missing and nothing else can be inferred from description\n",
    "mode_features = ['MSZoning', 'Utilities', 'Functional', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Electrical', 'KitchenQual']\n",
    "all_data.fillna({col:all_data[col][:ntrain].mode()[0] for col in mode_features}, inplace = True)\n",
    "\n",
    "#Fill LotFrontage with median value(consider only the training set) from the neighbourhood\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "    lambda x: x.fillna(x[:ntrain].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any remaining missing value ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check remaining missing values if any \n",
    "missing_data = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "missing_data = missing_data[missing_data != 0].sort_values(ascending=False)\n",
    "missing_data.rename('Percentage missing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value remains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming some numerical features to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some numerical features are actually really categories\n",
    "num_to_cat_features = ['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']\n",
    "all_data[num_to_cat_features] = all_data[num_to_cat_features].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we see how all numerical features are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the skew of each feature(consider only training set for finding skew)\n",
    "numerical_features = all_data.select_dtypes(exclude=['object']).columns\n",
    "skewed = all_data[numerical_features].apply(lambda x : skew(x[:ntrain])).sort_values(ascending=False)\n",
    "\n",
    "#Plot the skew of each feature\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x=skewed.index, y=skewed)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Features',fontsize=12)\n",
    "plt.ylabel('Skew', fontsize=12)\n",
    "plt.title('Skewness by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log transfrom the features whose absolute value of skew is >0.5. The value 0.5 is chosen as a rule of thumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features = skewed[abs(skewed)>0.5].index\n",
    "all_data[skewed_features] = np.log1p(all_data[skewed_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the skew after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = all_data[numerical_features].apply(lambda x : skew(x[:ntrain])).sort_values(ascending=False)\n",
    "\n",
    "#Plot the skew of each feature\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x=skewed.index, y=skewed)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Features',fontsize=12)\n",
    "plt.ylabel('Skew', fontsize=12)\n",
    "plt.title('Skewness by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features have been transformed such that their skew has been reduced significantly. The features in which high skew remains is due to most of the data having similar values and having some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is important so that the regularization penalizes every thing uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the features so that all of them lie in [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(all_data[numerical_features][:ntrain])\n",
    "all_data[numerical_features] = scaler.transform(all_data[numerical_features])\n",
    "\n",
    "#Check that data size is preserved.\n",
    "print(f'all_data size is : {all_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using one hot encoding as it does not make the dataset very big and will provide maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#Check that number of rows is preserved\n",
    "print(f'all_data size is : {all_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the train and test data\n",
    "X_train = all_data[:ntrain]\n",
    "y_train = train['SalePrice']\n",
    "X_test = all_data[ntrain:]\n",
    "\n",
    "#Check whether the datasets have correct size\n",
    "print(f'train size is : {X_train.shape}')\n",
    "print(f'target size is: {y_train.shape}')\n",
    "print(f'test size is : {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit a lasso model with builtin cross-validation to the training data. The advantage of lasso is that it increases model intrepretability by dropping unimportant features. But, first we have to find optimum penalty parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a plot of rmse vs alpha to find rough estimate of optimum alpha\n",
    "alphas = [0.0001*x for x in  range(1,11)]\n",
    "cv_score = [np.sqrt(-cross_val_score(LassoCV(alphas=[alpha], max_iter = 50000, cv=10), X_train, y_train, \n",
    "                                     scoring = 'neg_mean_squared_error', cv=10, n_jobs=4).mean()) for alpha in alphas]\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.lineplot(x=alphas, y = cv_score)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Variation of root mean squared error with penalty factor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum value of alpha is between 0.00035 and 0.00045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the lasso model using the optimum alpha\n",
    "lasso = LassoCV(alphas = [0.00035 + 0.00001*x for x in range(0,11)], max_iter = 50000, cv=10)\n",
    "lasso.fit(X_train,y_train)\n",
    "alpha = lasso.alpha_\n",
    "print(f'Optimum value of alpha is {round(alpha,5)}')\n",
    "\n",
    "#Find RMSE \n",
    "rmse = np.sqrt(-cross_val_score(LassoCV(alphas = [alpha]), X_train, y_train, scoring = 'neg_mean_squared_error', cv=10, n_jobs=4).mean())\n",
    "print(f'Root mean squared error on training set is {round(rmse,5)}')\n",
    "\n",
    "#Plot predictions\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(20,6))\n",
    "sns.regplot(x = y_train, y = lasso.predict(X_train), ax=ax1)\n",
    "ax1.set_title('Predicted price vs Actual price')\n",
    "ax1.set_xlabel('Actual values')\n",
    "ax1.set_ylabel('Predicted values')\n",
    "\n",
    "#Plot important coefficients\n",
    "coefs = pd.Series(lasso.coef_, index = X_train.columns)\n",
    "print(f'Lasso picked {sum(coefs != 0)} features and eliminated the other {sum(coefs == 0)} features')\n",
    "imp_coefs = coefs.reindex(coefs.abs().sort_values(ascending = False).index)[:10]\n",
    "sns.barplot(x=imp_coefs.index,y=imp_coefs.values, ax=ax2)\n",
    "ax2.set_title('Important features')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Coefficients')\n",
    "ax2.tick_params(axis='x', labelrotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to fit our model and submit the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = lasso.predict(X_test)\n",
    "submission = pd.DataFrame({'Id' : test_ID, 'SalePrice' : np.exp(y_test)-1})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
